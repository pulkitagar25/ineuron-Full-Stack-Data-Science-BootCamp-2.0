{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tExplain One-Hot Encoding\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "One hot encoding can be defined as the essential process of converting the categorical data variables to be provided to machine and deep learning algorithms which in turn improve predictions as well as classification accuracy of a model. One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This type of encoding creates a new binary feature for each possible category and assigns a value of 1 to the feature of each sample that corresponds to its original category. \n",
    "\n",
    "One hot encoding is a highly essential part of the feature engineering process in training for learning techniques. For example, we had our variables like colors and the labels were “red,” “green,” and “blue,” we could encode each of these labels as a three-element binary vector as Red: [1, 0, 0], Green: [0, 1, 0], Blue: [0, 0, 1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tExplain Bag of Words\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Bag of Words (BoW) is a representation technique for text data in natural language processing. It transforms text documents into numerical vectors that can be used as input to machine learning algorithms. In BoW, a text document is represented as a bag or multiset of words, disregarding grammar, order, or structure. Each unique word in the document is represented by a dimension in the numerical vector, and the value of the dimension is the frequency of the word in the document. This simple representation is useful in practice for tasks such as text classification, information retrieval, and clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tExplain Bag of N-Grams\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Bag of N-Grams (BoN) is an extension of the Bag of Words (BoW) representation technique for text data. While BoW only considers the frequency of individual words in a document, BoN considers sequences of words, called n-grams, as the basic building blocks. An n-gram is a contiguous sequence of n words in a document, and n can be any positive integer.\n",
    "\n",
    "In BoN, each unique n-gram in the document is represented by a dimension in the numerical vector, and the value of the dimension is the frequency of that n-gram in the document. By considering n-grams, BoN captures local relationships between words, preserving more of the context and structure of the text than BoW. However, increasing the value of n also increases the dimensionality of the representation, leading to the challenge of high-dimensional data and sparsity.\n",
    "\n",
    "BoN is commonly used in NLP tasks such as text classification, information retrieval, and language modeling."
   ]
  },
  {
   "attachments": {
    "download.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tExplain TF-IDF\n",
    "\n",
    "Ans=>\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that is used to reflect the importance of a word in a document relative to a corpus of documents. It is a common method for text feature extraction and has been widely used in information retrieval and natural language processing.\n",
    "\n",
    "TF-IDF is the product of two metrics:\n",
    "\n",
    "Term Frequency (TF): Represents the number of times a word appears in a document. The term frequency is calculated as the ratio of the number of times a word appears in a document to the total number of words in the document.\n",
    "\n",
    "Inverse Document Frequency (IDF): Represents the importance of a word across the corpus of documents. It is calculated as the logarithm of the ratio of the total number of documents in the corpus to the number of documents containing the word.\n",
    "\n",
    "The TF-IDF score of a word in a document reflects how frequently the word appears in the document, while also down-weighting the score for words that appear frequently across the corpus of documents. This allows words that are unique or distinctive to the document to be emphasized.\n",
    "\n",
    "In practice, TF-IDF is used to transform text into a numerical representation, which can then be used as input to machine learning algorithms for tasks such as text classification, information retrieval, and clustering.\n",
    "![download.jpg](attachment:download.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tWhat is OOV problem?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "OOV (Out-of-Vocabulary) is a problem that arises in natural language processing when the model encounters a word that was not present in its training data. The OOV problem occurs because most NLP models, such as bag-of-words and TF-IDF, rely on a fixed vocabulary, which is a pre-determined set of words that are deemed most important or relevant for the task at hand.\n",
    "\n",
    "When the model encounters a word that is not in the vocabulary, it is unable to make any predictions about it. This can lead to degraded performance on the NLP task, especially for low-frequency or rare words that are important for understanding the meaning of a text.\n",
    "\n",
    "To address the OOV problem, various techniques have been developed, such as using n-grams instead of individual words, using word embeddings to represent words as dense vectors, or using techniques like backoff or smoothing to generate predictions for OOV words based on their similarities to other words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tWhat are word embeddings?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Word embeddings are dense, continuous vector representations of words used in natural language processing. Unlike the traditional bag-of-words and TF-IDF representations, which only capture the frequency and presence of words in a text, word embeddings capture semantic and syntactic relationships between words.\n",
    "\n",
    "Word embeddings are learned by training a neural network on a large corpus of text, using a language modeling objective that predicts the next word in a sequence based on the previous words. During training, the neural network learns to map each word in the vocabulary to a dense, low-dimensional vector, such that semantically similar words are mapped to similar vectors. The resulting embeddings can then be used as input to other NLP models, such as text classification, information retrieval, and sequence labeling.\n",
    "\n",
    "Word embeddings have become a standard component in many NLP models and have been shown to improve performance on various tasks, due to their ability to capture the meaning and context of words in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tExplain Continuous bag of words (CBOW)\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Continuous Bag of Words (CBOW) is a method for learning word embeddings, which are dense, continuous vector representations of words used in natural language processing. The CBOW method is used to predict the target word in a context based on the surrounding words.\n",
    "\n",
    "In CBOW, the input to the model is a set of context words, and the output is the target word in the center of the context. The model learns to predict the target word by mapping the context words to a dense, low-dimensional vector, which can then be used to represent the target word. The mapping is learned through a neural network with a language modeling objective, which is trained on a large corpus of text.\n",
    "\n",
    "The resulting embeddings capture the meaning and context of words in a text, and are useful as input to other NLP models, such as text classification, information retrieval, and sequence labeling. The CBOW method is often contrasted with the Skip-Gram method, which predicts the surrounding words based on the target word in the center. Both CBOW and Skip-Gram can be used to learn word embeddings, and the choice between the two methods often depends on the specifics of the task and the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tExplain SkipGram\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Skip-Gram is a method for learning word embeddings, which are dense, continuous vector representations of words used in natural language processing. The Skip-Gram method is used to predict the surrounding words in a context based on the target word.\n",
    "\n",
    "In Skip-Gram, the input to the model is a target word, and the output is a set of context words surrounding the target word. The model learns to predict the context words by mapping the target word to a dense, low-dimensional vector, which can then be used to represent the target word. The mapping is learned through a neural network with a language modeling objective, which is trained on a large corpus of text.\n",
    "\n",
    "The resulting embeddings capture the meaning and context of words in a text, and are useful as input to other NLP models, such as text classification, information retrieval, and sequence labeling. The Skip-Gram method is often contrasted with the Continuous Bag of Words (CBOW) method, which predicts the target word based on the surrounding context words. Both Skip-Gram and CBOW can be used to learn word embeddings, and the choice between the two methods often depends on the specifics of the task and the available data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\tExplain Glove Embeddings.\n",
    "\n",
    "Ans=>\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is a method for learning word embeddings, which are dense, continuous vector representations of words used in natural language processing. GloVe is based on a co-occurrence matrix of words in a corpus of text, which measures the frequency with which word pairs appear together in the same context.\n",
    "\n",
    "The GloVe method learns the word embeddings by training a regression model to predict the logarithm of the co-occurrence count between words from their embeddings. The regression model is trained to minimize the difference between the predicted logarithm of the co-occurrence count and the actual logarithm of the co-occurrence count, as estimated from the co-occurrence matrix.\n",
    "\n",
    "The resulting embeddings capture both the meaning and context of words in a text, and have been shown to be useful as input to other NLP models, such as text classification, information retrieval, and sequence labeling. GloVe has become a popular method for learning word embeddings, due to its ability to capture global information about word co-occurrence, as well as its scalability to large corpora of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
