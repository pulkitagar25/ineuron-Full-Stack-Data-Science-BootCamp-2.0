{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tWhat are Vanilla autoencoders\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Vanilla autoencoders are a type of neural network used for unsupervised learning. They consist of an encoder and a decoder, where the encoder maps the input data to a lower-dimensional representation, and the decoder maps the lower-dimensional representation back to the original input data. The goal of training a vanilla autoencoder is to minimize the reconstruction error between the original input data and the output of the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tWhat are Sparse autoencoders\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Sparse autoencoders are a type of autoencoder that incorporate sparsity constraints into the training process. The idea is to encourage the encoder to produce a sparse representation of the input data, with most of the activations being zero. This can lead to a more efficient and interpretable representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tWhat are Denoising autoencoders\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Denoising autoencoders are a type of autoencoder that are trained to reconstruct the original input data from a noisy version of the input. The idea is to train the network to learn robust representations of the data that are invariant to noise and other types of corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tWhat are Convolutional autoencoders\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Convolutional autoencoders are a type of autoencoder that use convolutional neural networks (CNNs) as the encoder and decoder. They are used for image data and can learn compact representations of image patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tWhat are Stacked autoencoders\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Stacked autoencoders are a type of autoencoder that consist of multiple layers of encoders and decoders. The output of one layer is used as the input to the next layer, forming a hierarchical representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tExplain how to generate sentences using LSTM autoencoders\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "LSTM autoencoders are a type of autoencoder that use long short-term memory (LSTM) networks as the encoder and decoder. They are used for sequential data, such as text, and can learn to generate sequences that are similar to the input data. To generate sentences using LSTM autoencoders, one typically starts with a seed word, and then samples from the predicted distribution at each time step to generate the next word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tExplain Extractive summarization\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    " Extractive summarization is a type of text summarization that selects a subset of the most informative sentences from the original text to form a summary. The idea is to identify the most important sentences in the text and retain them in the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tExplain Abstractive summarization\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Abstractive summarization is a type of text summarization that generates a new summary by rewriting and condensing the original text. The idea is to capture the most important information in the text and present it in a concise and coherent way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\tExplain Beam search\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Beam search is a heuristic search algorithm used in natural language processing to generate sequences, such as text. It works by maintaining a set of K candidate sequences at each time step, and choosing the best K sequences from the set of candidates to generate the next set of candidates. This allows the algorithm to consider multiple alternatives at each step, instead of simply choosing the most likely candidate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\tExplain Length normalization\n",
    "\n",
    "Ans=>\n",
    "\n",
    " Length normalization is a technique used to adjust evaluation metrics for sequence generation tasks, such as machine translation and text summarization. The idea is to divide the evaluation metric by the length of the generated sequence to account for the fact that longer sequences tend to have higher values of the metric by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\tExplain Coverage normalization\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Coverage normalization is a technique used to mitigate over-generation in abstractive text summarization. It adds a coverage penalty term to the loss function used to train the model, so that the attention mechanism used by the model takes into account what portions of the input have already been summarized. This helps to prevent repetitive or redundant words from being generated in the output, and to encourage the model to consider the entire input text when generating the summary. The coverage penalty is typically calculated as a measure of the uniqueness of the attention weights assigned to each word in the input, or as a measure of how many times each word has already been attended to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\tExplain ROUGE metric evaluation\n",
    "\n",
    "Ans=>\n",
    "\n",
    "ROUGE ( Recall-Oriented Understudy for Gisting Evaluation) is a widely used evaluation metric for text summarization. It measures the overlap between a machine-generated summary and a reference or ground-truth summary. ROUGE evaluates the quality of summaries based on the number of overlapping n-grams (word sequences of length n) between the two summaries.\n",
    "\n",
    "ROUGE consists of multiple variants, including ROUGE-N, ROUGE-L, and ROUGE-W. Each variant focuses on a different aspect of the generated summary. For example, ROUGE-N measures the overlap in terms of unigrams (individual words), while ROUGE-L measures the longest common contiguous sequence (LCS) between the two summaries.\n",
    "\n",
    "ROUGE is often used to compare the performance of different summarization models and to tune the parameters of a particular model. High ROUGE scores indicate that the machine-generated summary is similar to the reference summary, while low scores indicate that the summary is dissimilar or poor quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
