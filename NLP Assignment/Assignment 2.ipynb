{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tWhat are Corpora?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "A corpus is a large and structured collection of texts used in natural language processing and computational linguistics. A corpus typically consists of a large number of documents, such as books, articles, and web pages, that are selected to represent a particular language or domain.\n",
    "\n",
    "Corpora are used to train and evaluate NLP models, such as machine translation systems, text classification models, and word embeddings. By training on a large and diverse corpus, NLP models can learn the patterns, structures, and statistical regularities of a particular language or domain. The use of corpora also allows for the development of general-purpose NLP models, which can be applied to a wide range of NLP tasks and domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tWhat are Tokens?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Tokens are the basic building blocks of NLP. They are the individual units that are extracted from a text for processing, such as words, punctuation marks, and numbers. Tokens are usually created by breaking a text into a sequence of basic units, and removing any unwanted characters or symbols.\n",
    "\n",
    "Once the text is tokenized, the tokens can be processed and analyzed by NLP models, such as machine translation systems, text classification models, and word embeddings. Tokens are often preprocessed, such as by lowercasing, stemming, or stop word removal, in order to improve the accuracy and efficiency of the NLP models.\n",
    "\n",
    "The choice of tokens depends on the NLP task and the domain of the text. For example, in some NLP tasks, such as sentiment analysis, it may be appropriate to use word-level tokens, while in others, such as part-of-speech tagging, it may be appropriate to use subword-level tokens, such as morphs or n-grams. The choice of tokens can have a significant impact on the performance and accuracy of NLP models, and is an important consideration in NLP preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tWhat are Unigrams, Bigrams, Trigrams?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Unigrams, bigrams, and trigrams are n-grams, which are sequences of n tokens used in NLP and computational linguistics. They are used as features in NLP models, such as text classification and information retrieval systems, in order to capture the context and meaning of words in a text.\n",
    "\n",
    "- Unigrams are individual tokens, such as words or subwords, that are extracted from a text.\n",
    "\n",
    "- Bigrams are sequences of two tokens that appear together in a text. They capture the relationship between adjacent words in a text, and are often used as features in NLP models.\n",
    "\n",
    "- Trigrams are sequences of three tokens that appear together in a text. They capture the relationship between three adjacent words in a text, and are used to provide more context and meaning than bigrams.\n",
    "\n",
    "The choice of n-grams depends on the NLP task and the domain of the text. N-grams are often used as features in NLP models, along with other features, such as word embeddings, to improve the accuracy and performance of the models. However, the use of higher-order n-grams, such as trigrams, can increase the feature space and computational complexity of the models, and must be carefully considered in NLP feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tHow to generate n-grams from text?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Generating n-grams from text involves splitting the text into individual tokens, and then creating sequences of n tokens (n-grams) from the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tExplain Lemmatization\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Lemmatization is a process in NLP that aims to reduce words to their base or root form, called the lemma. The lemma of a word is its dictionary form, which is often a more general or abstract form of the word. For example, the lemma of the word \"running\" might be \"run\".\n",
    "\n",
    "Lemmatization is different from stemming, which is another process in NLP that reduces words to their root form. Stemming is typically a more aggressive process that removes the suffix of a word, regardless of its meaning. This can result in non-real words, called stems, that are difficult to understand or interpret.\n",
    "\n",
    "Lemmatization, on the other hand, is a more sophisticated process that considers the morphological and semantic aspects of a word, and reduces it to its most basic form while preserving its meaning. This makes lemmatization a useful preprocessing step in NLP tasks, such as text classification and information retrieval, where the meaning of words is important.\n",
    "\n",
    "Lemmatization can be performed using morphological analysis, rule-based systems, or lexical resources, such as dictionaries or ontologies. It can also be performed using NLP libraries, such as NLTK or spaCy, that provide built-in lemmatization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tExplain Stemming\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Stemming is a process in NLP that aims to reduce words to their root form, called the stem. The stem of a word is a truncated version of the word that still retains its core meaning. For example, the stem of the word \"running\" might be \"run\".\n",
    "\n",
    "Stemming is different from lemmatization, which is another process in NLP that reduces words to their base or root form, called the lemma. The lemma of a word is its dictionary form, which is often a more general or abstract form of the word.\n",
    "\n",
    "Stemming is typically a more aggressive process that removes the suffix of a word, regardless of its meaning. This can result in non-real words, called stems, that are difficult to understand or interpret. However, stemming can be useful for NLP tasks, such as text classification and information retrieval, where the focus is on extracting the underlying meaning of the text, rather than preserving the exact words.\n",
    "\n",
    "Stemming can be performed using rule-based systems, such as the Porter stemmer or the Snowball stemmer, or using other algorithms, such as the Lovins stemmer or the Krovetz stemmer. Stemming can also be performed using NLP libraries, such as NLTK or scikit-learn, that provide built-in stemming functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tExplain Part-of-speech (POS) tagging\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of marking each word in a text with its corresponding grammatical category, such as noun, verb, adjective, adverb, etc. This process helps to identify the role that each word plays in a sentence and to analyze the grammatical structure of the text.\n",
    "\n",
    "POS tagging is an important step in NLP, and is used in various applications, such as information retrieval, text classification, and machine translation. For example, in information retrieval, POS tagging can be used to identify and extract relevant information, such as named entities or specific types of words, from the text.\n",
    "\n",
    "POS tagging can be performed using rule-based systems, such as the Brill tagger, or using statistical models, such as the Hidden Markov Model (HMM) or the Conditional Random Field (CRF). POS tagging can also be performed using NLP libraries, such as NLTK or spaCy, that provide built-in POS tagging functions.\n",
    "\n",
    "It's important to note that different languages may have different grammatical categories, and different NLP libraries may use different sets of tags for the same language. Therefore, it's important to choose an appropriate library and tag set for your specific NLP task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tExplain Chunking or shallow parsing\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Chunking, also known as shallow parsing, is the process of grouping words in a text into larger chunks, called chunks, based on their part-of-speech (POS) tags and grammatical relationships. Chunking is a way to identify and extract information from a text by grouping words into phrases, such as noun phrases, verb phrases, or adjective phrases.\n",
    "\n",
    "Chunking is a type of information extraction that is less complex and less time-consuming than full parsing, and provides a way to identify key elements of a sentence, such as the subject, verb, and object. This information can then be used for further analysis, such as named entity recognition, coreference resolution, or text classification.\n",
    "\n",
    "Chunking can be performed using rule-based systems, such as the RegexpParser in the NLTK library, or using statistical models, such as the Conditional Random Field (CRF) or the Hidden Markov Model (HMM). The choice of method will depend on the specific requirements of the NLP task and the quality of the training data.\n",
    "\n",
    "It's important to note that chunking is a shallow form of parsing and does not capture the full syntactic structure of the text. However, it is often sufficient for many NLP tasks, such as information extraction, where the goal is to identify specific elements of the text, rather than to analyze its full syntactic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\tExplain Noun Phrase (NP) chunking\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Noun Phrase (NP) chunking is a process in NLP that aims to identify and extract noun phrases from a text. A noun phrase is a group of words that refer to a single entity and typically consist of a noun and any associated words, such as adjectives, determiners, and prepositional phrases.\n",
    "\n",
    "NP chunking is a form of chunking or shallow parsing, which is the process of grouping words in a text into larger chunks based on their part-of-speech (POS) tags and grammatical relationships. NP chunking is used to identify the nouns in a sentence and any associated words that describe or modify the nouns.\n",
    "\n",
    "NP chunking can be performed using rule-based systems, such as the RegexpParser in the NLTK library, or using statistical models, such as the Conditional Random Field (CRF) or the Hidden Markov Model (HMM). The choice of method will depend on the specific requirements of the NLP task and the quality of the training data.\n",
    "\n",
    "NP chunking is an important step in many NLP tasks, such as information extraction, text classification, and named entity recognition, where the goal is to identify and extract specific elements of the text, such as named entities or key concepts. By identifying the nouns and any associated words in a text, NP chunking provides a way to extract and analyze the underlying meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\tExplain Named Entity Recognition\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Named Entity Recognition (NER) is a process in NLP that aims to identify and classify named entities, such as person names, organizations, locations, and dates, in a text. Named entities are specific, real-world items that can be easily referenced, and play a key role in many NLP tasks, such as information extraction, text classification, and question answering.\n",
    "\n",
    "NER is a type of information extraction that is performed by recognizing patterns in the text and classifying them into predefined categories, such as person names, organizations, locations, and dates. NER systems can be rule-based, using pattern matching and string matching techniques, or statistical, using machine learning models, such as Conditional Random Fields (CRF) or Recurrent Neural Networks (RNN).\n",
    "\n",
    "The accuracy of NER systems can vary greatly depending on the quality of the training data and the complexity of the NER task. It's also important to note that NER can be challenging in some domains, such as social media, where named entities may be misspelled, abbreviated, or written in non-standard ways.\n",
    "\n",
    "Overall, NER is a critical step in many NLP tasks, providing a way to identify and extract relevant information from a text, such as named entities, and to perform further analysis, such as entity disambiguation or text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
