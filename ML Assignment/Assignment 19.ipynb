{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "Ans=>\n",
    "\n",
    "a) With k = 2 and the first set of random centroids, the data points can be clustered as follows:\n",
    "\n",
    "Centroid 1: (15, 32)\n",
    "\n",
    "Cluster 1: 5, 10, 15, 20\n",
    "\n",
    "Centroid 2: (12, 30)\n",
    "\n",
    "Cluster 2: 25, 30, 35\n",
    "\n",
    "With k = 2 and the second set of random centroids, the data points can be clustered as follows:\n",
    "\n",
    "Centroid 1: (15, 32)\n",
    "\n",
    "Cluster 1: 25, 30, 35\n",
    "\n",
    "Centroid 2: (12, 30)\n",
    "\n",
    "Cluster 2: 5, 10, 15, 20\n",
    "\n",
    "b) To calculate the SSE (Sum of Squared Errors) for each set of centroids, we need to find the distance between each data point and its assigned centroid and then sum the squared distances.\n",
    "\n",
    "For the first set of centroids:\n",
    "\n",
    "Centroid 1: (15, 32)\n",
    "\n",
    "Cluster 1: 5, 10, 15, 20\n",
    "\n",
    "SSE = (5 - 15)^2 + (10 - 32)^2 + (15 - 15)^2 + (20 - 32)^2 = 100 + 144 + 0 + 144 = 388\n",
    "\n",
    "Centroid 2: (12, 30)\n",
    "\n",
    "Cluster 2: 25, 30, 35\n",
    "\n",
    "SSE = (25 - 12)^2 + (30 - 30)^2 + (35 - 12)^2 = 9 + 0 + 9 = 18\n",
    "\n",
    "Total SSE = 388 + 18 = 406\n",
    "\n",
    "For the second set of centroids:\n",
    "\n",
    "Centroid 1: (15, 32)\n",
    "\n",
    "Cluster 1: 25, 30, 35\n",
    "\n",
    "SSE = (25 - 15)^2 + (30 - 32)^2 + (35 - 15)^2 = 100 + 4 + 144 = 248\n",
    "\n",
    "Centroid 2: (12, 30)\n",
    "\n",
    "Cluster 2: 5, 10, 15, 20\n",
    "\n",
    "SSE = (5 - 12)^2 + (10 - 30)^2 + (15 - 12)^2 + (20 - 30)^2 = 49 + 256 + 9 + 100 = 414\n",
    "\n",
    "Total SSE = 248 + 414 = 662"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Market basket research makes use of association analysis concepts by analyzing the purchasing patterns of customers to identify which items are frequently bought together. The goal of this research is to find the most commonly occurring associations or co-occurrences of items in the customers' baskets. This information can then be used by retailers to improve their marketing and sales strategies, such as promoting complementary items or bundling products. The association analysis concepts used in market basket research are usually based on association rule mining algorithms such as the Apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Give an example of the Apriori algorithm for learning association rules.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "An example of the Apriori algorithm for learning association rules is as follows:\n",
    "Suppose we have a transaction database of seven transactions, where each transaction represents a customer's basket of items purchased.\n",
    "\n",
    "- T1: {Bread, Milk, Cheese}\n",
    "- T2: {Bread, Eggs}\n",
    "- T3: {Bread, Eggs, Cheese}\n",
    "- T4: {Bread, Milk, Cheese, Eggs}\n",
    "- T5: {Bread, Milk}\n",
    "- T6: {Eggs, Cheese}\n",
    "- T7: {Bread, Eggs, Cheese, Milk}\n",
    "\n",
    "We start by setting a minimum support threshold of 2 (i.e., an item set must occur in at least 2 transactions to be considered frequent).\n",
    "\n",
    "1. Generate all itemsets of size 1:\n",
    "\n",
    "- Bread: occurs in 5 transactions\n",
    "- Milk: occurs in 3 transactions\n",
    "- Cheese: occurs in 4 transactions\n",
    "- Eggs: occurs in 4 transactions\n",
    "\n",
    "2.Generate all itemsets of size 2 (i.e., pairs of items that occur together frequently):\n",
    "\n",
    "- Bread and Milk: occurs in 2 transactions\n",
    "- Bread and Cheese: occurs in 3 transactions\n",
    "- Bread and Eggs: occurs in 3 transactions\n",
    "- Milk and Cheese: occurs in 2 transactions\n",
    "- Milk and Eggs: occurs in 1 transaction\n",
    "- Cheese and Eggs: occurs in 2 transactions\n",
    "3. Generate all itemsets of size 3:\n",
    "\n",
    "- Bread, Milk, and Cheese: occurs in 2 transactions\n",
    "- Bread, Eggs, and Cheese: occurs in 2 transactions\n",
    "- Bread, Milk, and Eggs: occurs in 1 transaction\n",
    "\n",
    "4. Using the support values, we can find the association rules with confidence above the minimum confidence threshold (e.g., 50%):\n",
    "\n",
    "- Rule 1: Bread => Milk (support: 2, confidence: 2/5 = 40%)\n",
    "- Rule 2: Bread => Cheese (support: 3, confidence: 3/5 = 60%)\n",
    "- Rule 3: Bread => Eggs (support: 3, confidence: 3/5 = 60%)\n",
    "- Rule 4: Eggs => Cheese (support: 2, confidence: 2/4 = 50%)\n",
    "- Rule 5: Cheese => Eggs (support: 2, confidence: 2/4 = 50%)\n",
    "- Rule 6: Milk => Cheese (support: 2, confidence: 2/3 = 67%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In hierarchical clustering, the distance between clusters is measured using a distance metric such as Euclidean distance, Manhattan distance, or cosine similarity. This metric is used to determine the proximity of the clusters and decide when to merge them into a single cluster. The iteration stops when all clusters are merged into one big cluster or when the desired number of clusters is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In the k-means algorithm, the cluster centroids are recomputed at the end of each iteration by taking the average of all the data points assigned to each cluster. To do this, for each cluster, the sum of all the data points assigned to that cluster is calculated, and then divided by the number of points in that cluster. This gives the new cluster centroid. This process of computing new centroids and re-assigning data points to the closest cluster is repeated until the centroids stop changing, or until a maximum number of iterations is reached. This process helps to find the optimal partitioning of the data into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "One method for determining the required number of clusters is the elbow method. The elbow method involves plotting the sum of squared errors (SSE) versus the number of clusters and determining the \"elbow\" point on the plot. The elbow point represents the optimal number of clusters, as adding more clusters beyond this point does not result in significant reduction in SSE. The idea is that the SSE will decrease as the number of clusters increases, but at some point, the marginal decrease in SSE will not be substantial enough to warrant adding more clusters. The number of clusters at the elbow point is considered to be the optimal number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discuss the k-means algorithm's advantages and disadvantages.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Advantages of the k-means algorithm include:\n",
    "\n",
    "- Simplicity: The k-means algorithm is straightforward and easy to implement.\n",
    "- Speed: The k-means algorithm is fast for large datasets, as the computational complexity is linear with respect to the number of data points.\n",
    "- Scalability: The k-means algorithm can handle very large datasets, and can be easily parallelized.\n",
    "\n",
    "Disadvantages of the k-means algorithm include:\n",
    "\n",
    "- Sensitivity to Initial Centroids: The k-means algorithm is sensitive to the initial choice of centroids, and different initial centroids can result in different final clusters. To address this, the k-means algorithm can be run multiple times with different initial centroids and the best results can be chosen.\n",
    "- Assumes spherical clusters: The k-means algorithm assumes that clusters are spherical in shape, which may not always be the case in real-world datasets.\n",
    "- May not work well with non-linear datasets: The k-means algorithm is based on Euclidean distance, which may not work well with non-linear datasets."
   ]
  },
  {
   "attachments": {
    "images.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Draw a diagram to demonstrate the principle of clustering.\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The principle of clustering can be visualized as grouping similar data points together and separating dissimilar data points. A diagram to demonstrate this concept could be a scatter plot with different colors representing different clusters. The clusters are formed such that data points within a cluster are more similar to each other than to data points in other clusters. \n",
    "![images.png](attachment:images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. The clusters C1, C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The cluster centroids are the mean values of all the points in each cluster. In the first iteration, all the points in cluster C1 and C2 have the same values, so their centroids would also be the same, which is (4, 4). The centroid of cluster C3 is (7, 7).\n",
    "\n",
    "After the second iteration, the new cluster centroids are calculated by taking the average of all the points in each cluster. Since the points in C1 and C2 are unchanged, their centroids would remain the same as well. However, since the points in C3 have changed, its centroid would be different. The new centroid of cluster C3 would be the average of (5, 5) and (9, 9), which is (7, 7).\n",
    "\n",
    "The SSE (sum of squared errors) is a measure of how well the data points fit the centroid of their respective cluster. The SSE for this clustering would be zero, because all the data points are the same in C1 and C2, and the points in C3 are equidistant from its centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "A simple diagram that explains this process could be as follows:\n",
    "\n",
    "1. Defect data points collection: The first step is to collect all the 20 defect data points and store them in a data set.\n",
    "\n",
    "2. Text analytics: The next step is to perform text analytics on the defect details to extract relevant features that can be used for clustering. This could include extracting information such as the type of software component affected, the severity of the defect, and the cause of the defect.\n",
    "\n",
    "3. K-means algorithm: The next step is to apply the k-means algorithm on the data set to build 5 clusters of related defects. The algorithm takes the data set as input, along with the number of clusters (5 in this case), and outputs 5 cluster centroids and the data points assigned to each cluster.\n",
    "\n",
    "4. Cluster analysis: The team then analyzes the clusters and assigns a label to each cluster, based on the common characteristics of the defects within the cluster. For example, one cluster could be labeled as \"UI defects\", another could be labeled as \"Security defects\", and so on.\n",
    "\n",
    "5. New defect categorization: Any new defect that is discovered after the 5 clusters have been identified is then assigned to one of the 5 categories, based on its similarity to the existing clusters. The new defect is compared to the centroids of the 5 clusters, and the closest centroid is used to determine the category to which the new defect belongs.\n",
    "\n",
    "This process can be visualized as a flowchart, where the input is the 20 defect data points and the output is the 5 clusters of related defects. The process can be repeated as new defects are discovered, and the clusters can be updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
