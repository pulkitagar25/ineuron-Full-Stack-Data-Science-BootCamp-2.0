{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The underlying concept of Support Vector Machines (SVMs) is to find the best boundary (or hyperplane) that separates the data into two classes. This boundary is chosen such that the distance between the boundary and the closest data points (also known as support vectors) is maximized. These support vectors are used to define the boundary and are used to make predictions about new cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What is the concept of a support vector?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The concept of a support vector refers to the data points that are closest to the boundary and define the boundary. These data points have the greatest influence on the boundary and are used to make predictions about new cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "When using SVMs, it is necessary to scale the inputs because the SVM algorithm uses the distance between data points to determine the boundary. If the inputs are not scaled, the SVM algorithm may give more importance to features with larger values, leading to a bias in the boundary. Scaling the inputs ensures that all features are treated equally, and the SVM algorithm finds the optimal boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "When an SVM classifier classifies a case, it can output a confidence score indicating how certain the classifier is about its prediction. The confidence score is based on the distance between the test case and the boundary. The closer the test case is to the boundary, the lower the confidence score. However, SVM classifiers do not output a percentage chance of the test case belonging to each class. This is because SVMs are designed to classify cases into two classes, and the decision boundary is a line or a hyperplane that separates the two classes. The SVM algorithm does not provide a probabilistic interpretation of the classifier's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The choice between the primal and dual form of the SVM problem depends on the size of the training set and the number of features. If the training set is small and the number of features is large, the primal form is preferred because it is faster to solve. On the other hand, if the training set is large and the number of features is small, the dual form is preferred because it is more efficient in terms of memory usage. In the case of a training set with millions of instances and hundreds of features, the dual form of the SVM problem is likely to be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "If an RBF kernel SVM classifier underfits the training collection, it is better to raise the gamma parameter. A high gamma value results in a more flexible decision boundary and can help to capture more complex relationships in the data. As for the letter C, increasing the value of C will result in a more complex boundary, which can also help to address underfitting. However, increasing C too much can result in overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, the QP parameters (H, f, A, and b) should be set as follows:\n",
    "\n",
    "- H is the gram matrix of the data, which is the dot product of the data points.\n",
    "- f is a vector of all -1's.\n",
    "- A is a matrix with one row for each data point, and the elements in each row are either 0 or 1 depending on the label of the data point.\n",
    "- b is a vector of all 0's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "To train a model similar to a LinearSVC on a linearly separable dataset, the SVC and SGDClassifier models should be trained with a linear kernel. For the SVC model, set the C parameter to a large value to produce a complex boundary that separates the data into the correct classes. For the SGDClassifier model, set the loss parameter to \"hinge\" and adjust the alpha parameter to control the complexity of the boundary. In both cases, the goal is to produce a boundary that separates the data into the correct classes with a large margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The level of precision that can be achieved with an SVM classifier on the MNIST dataset depends on several factors, including the choice of kernel, the value of the hyperparameters, and the size of the validation set used for tuning. With appropriate hyperparameter tuning and a suitable kernel, it is possible to achieve high precision with an SVM classifier on the MNIST dataset.\n",
    "\n",
    "In practice, SVM classifiers have been known to achieve precision levels of around 95-98% on the MNIST dataset. However, this is still dependent on the specific implementation, and results may vary. In order to achieve the best possible results, it is important to carefully choose the kernel and hyperparameters, and to perform a thorough evaluation of the model using cross-validation and/or a separate test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "Ans=>\n",
    "\n",
    "To train an SVM regressor on the California housing dataset, you'll need to use the SVM regression algorithm, which is a type of SVM designed for regression tasks. Here is a high-level outline of the steps involved:\n",
    "\n",
    "1. Prepare the data: Load the California housing dataset and split it into training and testing sets.\n",
    "\n",
    "2. Preprocessing: Scale the features of the data to ensure that they are on a similar scale, as this can have a significant impact on the performance of an SVM regressor.\n",
    "\n",
    "3. Choose a kernel: Select a kernel function to use in the SVM regressor. Common choices for regression tasks include radial basis function (RBF) and polynomial kernels.\n",
    "\n",
    "4. Choose hyperparameters: Decide on the value of the regularization parameter (C) and the kernel parameter (gamma) to use in the SVM regressor.\n",
    "\n",
    "5. Train the SVM regressor: Fit the SVM regressor to the training data using the chosen kernel and hyperparameters.\n",
    "\n",
    "6. Evaluate the model: Use the trained SVM regressor to make predictions on the test data and evaluate its performance using metrics such as mean squared error (MSE) or root mean squared error (RMSE).\n",
    "\n",
    "7. Refine the model: If necessary, adjust the hyperparameters and repeat the training process to improve the performance of the SVM regressor.\n",
    "\n",
    "It is important to note that the level of precision that can be achieved with an SVM regressor on the California housing dataset will depend on a variety of factors, including the choice of kernel, the values of the hyperparameters, and the quality of the training data. In order to achieve the best possible results, it is important to carefully choose the kernel and hyperparameters and to perform a thorough evaluation of the model using cross-validation and/or a separate test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
