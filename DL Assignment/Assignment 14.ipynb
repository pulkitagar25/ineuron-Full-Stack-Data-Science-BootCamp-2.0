{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tIs it okay to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "No, it is not generally a good idea to initialize all the weights to the same value, even if the value is selected randomly using He initialization. If all the weights are the same, then all the neurons in the same layer will produce the same output, regardless of their inputs. This can lead to a lack of diversity in the network's representation of the data, and can make it more difficult for the network to learn complex features. Instead, it is recommended to initialize the weights with random values drawn from a distribution that has zero mean and a small variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tIs it okay to initialize the bias terms to 0?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Yes, it is generally okay to initialize the bias terms to 0, since the effect of the biases can be adjusted during training. However, it is possible that initializing the biases to a non-zero value may help the network converge faster or achieve better performance, depending on the specifics of the problem and the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tName three advantages of the ELU activation function over ReLU.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Three advantages of the ELU (Exponential Linear Unit) activation function over ReLU (Rectified Linear Unit) are:\n",
    "\n",
    "- ELU can handle negative inputs better than ReLU, by allowing a small negative value for the output when the input is negative. This can help avoid the \"dying ReLU\" problem, where some neurons may become permanently inactive during training because their weights are updated in a way that always keeps their output below zero.\n",
    "- ELU can produce smoother gradients than ReLU, which can help the network converge faster and avoid the vanishing gradient problem, where the gradients become very small and prevent the weights from being updated effectively.\n",
    "- ELU can produce negative outputs, which can help the network represent negative relationships between features in the data. ReLU can only produce non-negative outputs, which can limit the expressiveness of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tIn which cases would you want to use each of the following activation functions: ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Here are some guidelines for when to use different activation functions:\n",
    "- ELU (Exponential Linear Unit): ELU is a good default choice for most cases, since it has several advantages over other activation functions, including better handling of negative inputs and smoother gradients. However, it may be slightly slower to compute than some other activation functions.\n",
    "- Leaky ReLU: If the ReLU activation function causes some neurons to always output 0, or if the learning rate needs to be reduced to avoid this problem, then a leaky ReLU may be a better choice. Leaky ReLU allows a small negative output when the input is negative, which can help avoid the \"dying ReLU\" problem.\n",
    "- ReLU (Rectified Linear Unit): ReLU is a good default choice if you don't want to use ELU, and it can be faster to compute than some other activation functions. However, it can suffer from the \"dying ReLU\" problem, especially in deeper networks or on data with many negative inputs.\n",
    "- tanh (hyperbolic tangent): tanh can be a good choice for hidden layers, especially if the input data is standardized to have a mean of 0 and a standard deviation of 1. It produces values between -1 and 1, which can help keep the gradients in a good range for optimization.\n",
    "- logistic (sigmoid): logistic can be a good choice for the output layer of a binary classification problem, where the goal is to produce a probability of the positive class. However, it is less common to use logistic as an activation function for hidden layers, since it can suffer from the vanishing gradient problem and is slower to compute than other activation functions.\n",
    "- softmax: softmax is typically used as the activation function for the output layer in multiclass classification problems, where the goal is to produce a probability distribution over the possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a MomentumOptimizer?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "If the momentum hyperparameter is set too close to 1 when using a MomentumOptimizer, then the optimizer may overshoot the minimum of the loss function and start oscillating back and forth. This can lead to slow convergence or even failure to converge. A momentum value of around 0.9 is typically a good starting point, and the value should be adjusted based on the specific problem and network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tName three ways you can produce a sparse model.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Three ways to produce a sparse model are:\n",
    "\n",
    "- L1 regularization: L1 regularization adds a penalty term to the loss function that encourages the model weights to be close to zero. This can result in a sparse model, where many of the weights are exactly zero.\n",
    "- Dropout: Dropout randomly sets some of the inputs to a neuron to zero during training. This can encourage the network to learn redundant representations, since it cannot rely on any single input. The resulting model can be sparse, since many of the neurons will be inactive on any given input.\n",
    "- Weight pruning: Weight pruning involves setting small weights to exactly zero, and then retraining the network to adjust the remaining weights. This can result in a sparse model, where many of the weights are exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Dropout can slow down training, since it effectively reduces the capacity of the network by randomly removing some neurons during each training step. This can increase the number of training steps required to reach a good solution. However, it can also help prevent overfitting, which can ultimately lead to faster convergence and better performance. Dropout does not slow down inference, since all neurons are active during inference and their outputs are scaled to compensate for the dropout during training. In fact, dropout can even speed up inference by reducing the amount of redundant computation required by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
