{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tWrite the Python code to implement a single neuron.\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, n_inputs):\n",
    "        self.weights = np.random.randn(n_inputs)\n",
    "        self.bias = np.random.randn()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = np.dot(self.weights, inputs) + self.bias\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tWrite the Python code to implement ReLU.\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tWrite the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2 / input_size) # He initialization\n",
    "        self.biases = np.zeros(output_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, d_outputs, learning_rate):\n",
    "        d_weights = np.dot(self.inputs.T, d_outputs)\n",
    "        d_biases = np.sum(d_outputs, axis=0)\n",
    "        d_inputs = np.dot(d_outputs, self.weights.T)\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.biases -= learning_rate * d_biases\n",
    "        return d_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tWrite the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(x, W, b):\n",
    "    \"\"\"\n",
    "    Computes the output of a dense layer given an input, a weight matrix, and a bias vector.\n",
    "    Args:\n",
    "        x: Input vector (shape [batch_size, input_size])\n",
    "        W: Weight matrix (shape [input_size, output_size])\n",
    "        b: Bias vector (shape [output_size])\n",
    "    Returns:\n",
    "        Output vector (shape [batch_size, output_size])\n",
    "    \"\"\"\n",
    "    output = [[sum([x[i][j] * W[j][k] for j in range(len(x[0]))]) + b[k] for k in range(len(b))] for i in range(len(x))]\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tWhat is the “hidden size” of a layer?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The \"hidden size\" of a layer in a neural network refers to the number of neurons in that layer. It is called \"hidden\" because this information is not visible from the input or output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tWhat does the t method do in PyTorch?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The t() method in PyTorch is used to transpose a tensor. It returns a new tensor with the dimensions reversed, such that the rows of the input become the columns of the output, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tWhy is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Matrix multiplication written in plain Python is very slow because it involves a lot of looping over the elements of the matrices, which is inefficient in Python. Matrix multiplication can be written much more efficiently using linear algebra libraries like NumPy or specialized hardware like GPUs. These libraries and hardware can perform the multiplication much more quickly by taking advantage of parallelization and optimized low-level code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tIn matmul, why is ac==br?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In matmul, ac==br because the matrices need to be compatible for matrix multiplication, which means that the number of columns in the first matrix must be equal to the number of rows in the second matrix. If ac is not equal to br, then the matrices are not compatible for multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\tIn Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In Jupyter Notebook, you can use the %timeit magic command to measure the time taken for a single cell to execute. Simply type %timeit at the beginning of the cell, followed by the code you want to time. When you run the cell, the command will automatically run the code several times and print out the average execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\tWhat is elementwise arithmetic?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Elementwise arithmetic is a mathematical operation applied independently to each element of an array or tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\tWrite the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 1, 2])\n",
    "\n",
    "all_greater = torch.all(a > b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\tWhat is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(42)\n",
    "x_as_python = x.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.\tHow does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Elementwise arithmetic can help speed up matmul by allowing us to perform operations elementwise (such as addition, multiplication, etc.) before or after the matrix multiplication. By doing so, we can reduce the number of operations required and simplify the computation. For example, we can add a bias vector to a matrix by performing an elementwise addition instead of adding the bias vector to each row of the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.\tWhat are the broadcasting rules?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Broadcasting is a feature in PyTorch that allows us to perform elementwise operations on tensors with different shapes. The broadcasting rules determine how PyTorch evaluates elementwise operations between two tensors with different shapes. The rules are as follows:\n",
    "\n",
    "- If the two tensors have the same number of dimensions, but the size of any dimension does not match, PyTorch will try to broadcast the tensor with the smaller size to the size of the larger tensor by adding dimensions of size 1 at the beginning (left padding).\n",
    "- If the two tensors have different number of dimensions, PyTorch will add dimensions of size 1 to the smaller tensor to match the number of dimensions of the larger tensor, and then apply the rule above.\n",
    "- If the two tensors cannot be broadcast together (i.e., their sizes are not compatible), PyTorch will raise a RuntimeError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.\tWhat is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create tensors\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([[2], [4], [6]])\n",
    "\n",
    "# add tensors using broadcasting\n",
    "c = a + b\n",
    "print(c)\n",
    "\n",
    "# add tensors using expand_as to match broadcasting shape\n",
    "d = a.expand_as(b) + b\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
