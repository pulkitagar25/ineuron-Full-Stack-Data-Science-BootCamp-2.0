{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tWhat does a SavedModel contain? How do you inspect its content?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "A SavedModel contains a trained TensorFlow model and its associated assets, such as the model's architecture, weights, optimizer state, and metadata. It is a universal format that can be used to deploy models across various platforms, including TensorFlow Serving, TensorFlow Lite, and TensorFlow.js.\n",
    "To inspect the content of a SavedModel, you can use the saved_model_cli command-line interface provided by TensorFlow. For example, you can use the following command to list the available signatures and tags of a SavedModel:\n",
    "\n",
    "saved_model_cli show --dir /path/to/saved_model --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tWhen should you use TF Serving? What are its main features? What are some tools you can use to deploy it?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "You should use TF Serving when you want to deploy a TensorFlow model in a production environment that requires scalability, low latency, and high availability. TF Serving is a TensorFlow component that provides a server that can serve multiple models simultaneously and handle multiple requests in parallel. It allows you to deploy models as microservices and perform efficient batching, caching, and dynamic loading of models.\n",
    "TF Serving's main features include:\n",
    "\n",
    "- Support for multiple model formats, including SavedModel, TensorFlow Hub modules, and TensorFlow Lite models\n",
    "- Efficient batching of requests and responses\n",
    "- Dynamic loading and unloading of models without downtime\n",
    "- Health checking and monitoring of server and models\n",
    "- Integration with Kubernetes for deployment and scaling\n",
    "- Support for TensorFlow Serving API and gRPC protocol\n",
    "\n",
    "To deploy a TensorFlow model using TF Serving, you need to export the model as a SavedModel and start a TF Serving server with the appropriate configurations, such as the model's signature, input and output keys, and port number. You can use tools such as Docker, Kubernetes, or Ansible to automate the deployment process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tHow do you deploy a model across multiple TF Serving instances?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "To deploy a model across multiple TF Serving instances, you can use the TensorFlow Model Garden's HorovodRunner. HorovodRunner is a tool that provides distributed training and inference capabilities for TensorFlow models using the Horovod framework. It allows you to parallelize the training or inference of a model across multiple nodes or GPUs using data parallelism or model parallelism.\n",
    "To use HorovodRunner for deployment, you need to wrap your model in a TensorFlow Estimator, which defines the training or inference process of the model. Then, you can create a HorovodRunner object and pass it the Estimator, along with the required configurations for distributed training or inference, such as the number of nodes, GPUs, and batch sizes. Finally, you can call the HorovodRunner's train() or predict() method to start the distributed process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tWhen should you use the gRPC API rather than the REST API to query a model served by TF Serving?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The gRPC API is recommended over the REST API for high-performance and low-latency use cases, especially when transferring large amounts of data between the client and the server. This is because the gRPC API uses a binary protocol instead of text-based requests and responses, which reduces the size of the messages and improves communication speed. The gRPC API also supports bi-directional streaming, which allows for more efficient real-time communication between the client and server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tWhat are the different ways TFLite reduces a modelâ€™s size to make it run on a mobile or embedded device?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "TFLite reduces a model's size to make it run on a mobile or embedded device in several ways, including:\n",
    "\n",
    "- Quantization: TFLite converts the weights and activations of the model from 32-bit floating point precision to 8-bit fixed point precision, reducing the model size and increasing inference speed.\n",
    "- Operator fusion: TFLite combines multiple operations into a single operation, reducing the number of operations required to run the model and improving inference speed.\n",
    "- Weight pruning: TFLite removes the least important weights from the model, reducing the model size and improving inference speed.\n",
    "- Model compression: TFLite uses techniques such as Huffman coding and arithmetic coding to compress the model weights and activations, reducing the model size without sacrificing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tWhat is quantization-aware training, and why would you need it?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Quantization-aware training is a technique for training neural networks that are more suitable for deployment on mobile or embedded devices. The technique involves training the network with quantization in mind, so that the network can be quantized directly after training without losing too much accuracy. This is achieved by simulating quantization during training, so that the network can learn to be more robust to the quantization process. Quantization-aware training is useful because it allows neural networks to be deployed on devices with limited computational resources, such as mobile phones and IoT devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tWhat are model parallelism and data parallelism? Why is the latter generally recommended?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Model parallelism and data parallelism are two strategies for parallelizing the training of deep learning models across multiple devices. In model parallelism, the model is partitioned into parts that are trained on different devices. In data parallelism, the same model is trained on different devices with different mini-batches of data. The latter is generally recommended because it is easier to implement, more flexible, and more efficient in terms of communication costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tWhen training a model across multiple servers, what distribution strategies can you use? How do you choose which one to use?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "When training a model across multiple servers, there are several distribution strategies that can be used, including:\n",
    "\n",
    "- MirroredStrategy: replicates the model on each device and trains it with different mini-batches of data, synchronizing the gradients across devices to update the model.\n",
    "- ParameterServerStrategy: partitions the model and trains it on multiple devices, using a separate set of parameter servers to store and update the model parameters.\n",
    "- MultiWorkerMirroredStrategy: similar to MirroredStrategy, but with multiple workers that can train on different machines, each with its own set of GPUs.\n",
    "\n",
    "The choice of distribution strategy depends on the specific training scenario, including the size of the model, the number of devices available, the communication bandwidth between devices, and the trade-off between computation and communication costs. It is important to benchmark different strategies to determine the optimal one for the given scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
