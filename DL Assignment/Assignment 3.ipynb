{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Initializing all weights to the same value using He initialization is not recommended. He initialization is designed to provide a suitable scale for the initial weights, but it is still important to have some level of randomization to break the symmetry of the weights. Therefore, He initialization typically initializes the weights with a random Gaussian distribution with mean 0 and standard deviation sqrt(2/n), where n is the number of inputs to the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tIs it OK to initialize the bias terms to 0?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "It is generally fine to initialize bias terms to 0, especially for ReLU activation functions, which are commonly used in deep learning. This is because the ReLU function is already zero-centered, so the initial bias does not have a significant effect on the output of the neuron. However, for other activation functions that are not zero-centered, such as sigmoid or tanh, initializing the bias to 0 could lead to slower convergence or other issues. In general, it's a good practice to initialize biases randomly, similar to how the weights are initialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tName three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Three advantages of the SELU (Scaled Exponential Linear Unit) activation function over the ReLU (Rectified Linear Unit) activation function:\n",
    "\n",
    "1.  Self-normalization: The SELU activation function is designed to preserve the mean and variance of the activations across layers, which helps avoid the vanishing gradient problem and promotes faster and more stable convergence. This means that for deep neural networks with many layers, using SELU activation can improve performance over ReLU.\n",
    "\n",
    "2. Continuous and differentiable: The SELU function is a continuous and differentiable function, which means that it is easier to compute gradients and use optimization algorithms that rely on gradients, such as backpropagation.\n",
    "\n",
    "3. No need for bias term: The SELU function has a self-regulating property that allows it to automatically adjust the mean and variance of the activations to a certain range. This means that in some cases, using the SELU function can eliminate the need for a bias term in the neural network, which can reduce the number of parameters and make the network easier to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Different activation functions are suited for different scenarios based on their properties. Here are some common use cases for each of the activation functions:\n",
    "\n",
    "1. SELU: SELU is a good choice for deep neural networks due to its ability to maintain mean activation close to 0 and standard deviation close to 1, thereby avoiding the vanishing and exploding gradient problems that can arise in deep networks. It also tends to perform well in image classification tasks.\n",
    "\n",
    "2. Leaky ReLU: Leaky ReLU is useful when you want to avoid the \"dying ReLU\" problem, where certain neurons in the network never activate and therefore never learn. It can be particularly useful in training deep networks with a large number of layers.\n",
    "\n",
    "3. ReLU: ReLU is a commonly used activation function and often the default choice for many deep learning models. It is useful for training deep networks, and its simple implementation and computational efficiency make it a popular choice.\n",
    "\n",
    "4. tanh: tanh can be used in situations where you want the output to be bounded between -1 and 1, which can be useful in certain types of data normalization. It can also be useful in image classification tasks.\n",
    "\n",
    "5. logistic: logistic activation function is commonly used in binary classification problems, where the output of the network needs to be between 0 and 1.\n",
    "\n",
    "6. softmax: softmax is used in the output layer of a neural network when performing multi-class classification. It ensures that the output of the network is a probability distribution over the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) when using a stochastic gradient descent (SGD) optimizer can lead to the following issues:\n",
    "\n",
    "1. The optimizer may oscillate around the minimum or fail to converge: The momentum term helps the optimizer to move in the right direction by adding a fraction of the previous update vector to the current update. However, if the momentum is too high, the optimizer may overshoot the minimum and continue to oscillate around it without converging.\n",
    "\n",
    "2. The model may fail to generalize: High momentum values can cause the optimizer to get stuck in shallow local minima, preventing the model from finding a better global minimum. This may lead to overfitting on the training data and poor generalization performance on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tName three ways you can produce a sparse model.\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Here are three ways to produce a sparse model:\n",
    "\n",
    "1. L1 regularization: This technique adds an L1 penalty term to the cost function, which encourages the model to use fewer features by shrinking the weights of unimportant features to zero. This results in a sparse model where many features have zero weights.\n",
    "\n",
    "2. Dropout regularization: Dropout randomly drops out some of the neurons during training, which encourages the remaining neurons to learn more robust features that are useful for making predictions. This can lead to sparsity in the learned weights.\n",
    "\n",
    "3. Pruning: Pruning is a technique that involves removing unimportant weights from a trained model to make it more compact and efficient. By setting small weights to zero and removing the corresponding connections, the model can be made more sparse. This can be done using various methods such as magnitude-based pruning, weight-level pruning, and neuron-level pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Yes, dropout can slow down training, as it effectively removes some neurons randomly during each training step, which can require more iterations to converge. However, dropout can also have a regularization effect, reducing overfitting and improving the performance of the model on unseen data.\n",
    "\n",
    "During inference (i.e., making predictions on new instances), dropout does not have any effect, as it is only applied during training. The dropout layers can simply be turned off during inference.\n",
    "\n",
    "MC Dropout, which involves applying dropout at inference time and averaging the predictions across multiple runs, can be slower than regular inference, as it requires running the model multiple times. However, it can lead to more accurate predictions and better uncertainty estimates, making it a useful tool for certain applications.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
