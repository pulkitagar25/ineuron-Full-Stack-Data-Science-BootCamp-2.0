{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explain the Activation Functions in your own language\n",
    "a)\tsigmoid\n",
    "b)\ttanh\n",
    "c)\tReLU\n",
    "d)\tELU\n",
    "e)\tLeakyReLU\n",
    "f)\tswish\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "a) Sigmoid: The sigmoid function is a smooth S-shaped curve that squashes the input values between 0 and 1. It is commonly used as an activation function in binary classification problems where the output needs to be a probability value between 0 and 1.\n",
    "\n",
    "b) Tanh: The hyperbolic tangent (tanh) function is similar to the sigmoid function but squashes the input values between -1 and 1. It is also a smooth curve and is commonly used as an activation function in hidden layers of neural networks.\n",
    "\n",
    "c) ReLU: The rectified linear unit (ReLU) activation function is a piecewise linear function that returns the input if it is positive, and 0 if it is negative. It is a popular activation function because it is computationally efficient and has been shown to work well in many deep learning architectures.\n",
    "\n",
    "d) ELU: The exponential linear unit (ELU) activation function is a modified version of ReLU that has a smooth exponential curve for negative input values. This allows the activation function to have negative values, which can be helpful in certain cases, such as reducing the vanishing gradient problem.\n",
    "\n",
    "e) LeakyReLU: The leaky rectified linear unit (LeakyReLU) activation function is similar to ReLU, but allows a small, non-zero gradient for negative input values. This can help avoid the \"dying ReLU\" problem, where ReLU neurons can become permanently inactive during training.\n",
    "\n",
    "f) Swish: The swish activation function is a newer activation function that has been shown to work well in some deep learning architectures. It is a smooth curve that is similar to a sigmoid function, but with a linear component at the high end. The output of the function is the input multiplied by the sigmoid of the input, which can help preserve information from both positive and negative inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What happens when you increase or decrease the optimizer learning rate?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "When you increase the learning rate of an optimizer, the model's weight updates will be larger, which can result in faster convergence during training. However, if the learning rate is set too high, the optimizer may overshoot the optimal values and cause the model to diverge. Conversely, if you decrease the learning rate, the weight updates will be smaller, and the model will converge more slowly. Finding the right learning rate is essential to balancing convergence speed and accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What happens when you increase the number of internal hidden neurons?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Increasing the number of internal hidden neurons in a neural network can increase the model's representational capacity, allowing it to learn more complex relationships in the data. However, adding too many neurons can lead to overfitting, as the model may begin to memorize the training data instead of learning generalizable patterns. Therefore, adding more neurons should be done carefully, and the model should be monitored for signs of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What happens when you increase the size of batch computation?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "When you increase the batch size, the model processes more training examples in each iteration, which can result in faster training times. However, larger batch sizes can also require more memory and computational resources, and may lead to poorer generalization performance if the model overfits to the training data. Additionally, larger batch sizes may result in a less noisy estimate of the gradient, which can smooth out the training trajectory and potentially lead to slower convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Why we adopt regularization to avoid overfitting?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Regularization techniques such as L1, L2, and dropout are used to prevent overfitting by adding a penalty term to the loss function or randomly dropping out some of the neurons during training. Regularization encourages the model to learn simpler, smoother decision boundaries, which can generalize better to unseen data. Without regularization, the model may become overly complex and fit the noise in the training data, leading to poor performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What are loss and cost functions in deep learning?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In deep learning, a loss function (also known as a cost function or objective function) is a measure of how well the model is performing on the training data. It quantifies the difference between the predicted outputs of the model and the actual outputs (labels) for each training example. The goal of the training process is to minimize the value of the loss function, which means that the model's predictions are closer to the true labels. Popular loss functions include mean squared error (MSE) for regression problems, binary cross-entropy for binary classification, and categorical cross-entropy for multiclass classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What do ou mean by underfitting in neural networks?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Underfitting is a situation in which a machine learning model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test data. In the case of neural networks, underfitting occurs when the model has too few hidden units or layers and cannot learn the complex relationships in the data. This may be due to a lack of training data or insufficient training time. Underfitting is characterized by high training and test error and can be addressed by increasing the model's capacity (adding more layers or neurons), training for more epochs, or providing more training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Why we use Dropout in Neural Networks?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Dropout is a regularization technique used in neural networks to prevent overfitting. It works by randomly dropping out (setting to zero) some of the neurons in the network during each training iteration. This forces the remaining neurons to learn more robust and independent features, as they can no longer rely on the presence of specific co-adapted neurons. Dropout is typically applied to the hidden layers of the network and has been shown to improve generalization performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
