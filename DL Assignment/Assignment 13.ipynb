{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tWhy is it generally preferable to use a Logistic Regression classifier rather than a classical Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression classifier?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Logistic regression is generally preferred over a classical perceptron for several reasons:\n",
    "\n",
    "- Probabilistic outputs: Logistic regression outputs a probability score for each class, which can be interpreted as the confidence of the model in its prediction. This is particularly useful for classification problems where it is important to have an estimate of the uncertainty associated with the prediction.\n",
    "\n",
    "- Non-linear decision boundary: Logistic regression can model non-linear decision boundaries between classes, whereas a classical perceptron can only model linear decision boundaries. This can be a significant limitation in many real-world classification problems.\n",
    "\n",
    "- Differentiable loss function: The logistic regression loss function is differentiable, which means it can be optimized using gradient-based methods. This allows for more efficient training and better convergence properties compared to the Perceptron training algorithm, which uses a step function to update weights.\n",
    "\n",
    "To make a perceptron equivalent to a logistic regression classifier, one can use the following modifications:\n",
    "\n",
    "- Replace the step function with a sigmoid function or another activation function that is differentiable.\n",
    "\n",
    "- Modify the output of the perceptron to produce a probability score for each class. This can be done using the softmax function, which normalizes the output of the perceptron over all classes and produces a probability distribution over the classes.\n",
    "\n",
    "By making these modifications, the perceptron becomes a single-layer neural network with a sigmoid or other activation function and a softmax output, which is equivalent to a logistic regression classifier.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tWhy was the logistic activation function a key ingredient in training the first MLPs?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The logistic activation function was a key ingredient in training the first MLPs because it is a differentiable, non-linear function that is well-suited for backpropagation. The logistic function produces output values that are bounded between 0 and 1, which is useful for modeling probabilities or binary outcomes in classification problems. Additionally, the logistic function is sigmoidal, which means it has a smooth gradient that facilitates efficient training through backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tName three popular activation functions. Can you draw them?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Three popular activation functions in neural networks are:\n",
    "\n",
    "a. ReLU (Rectified Linear Unit): The ReLU function is defined as f(x) = max(0, x), which means that the output is 0 if the input is negative, and the output is equal to the input if the input is positive. The ReLU function is popular because it is simple, fast to compute, and has been shown to be effective in many types of neural networks.\n",
    "\n",
    "\n",
    "b. Sigmoid: The sigmoid function is defined as f(x) = 1 / (1 + exp(-x)), which produces output values between 0 and 1. The sigmoid function is useful for modeling probabilities or binary outcomes in classification problems. However, the sigmoid function can suffer from vanishing gradients for large or small inputs.\n",
    "\n",
    "c. Tanh (hyperbolic tangent): The tanh function is defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)), which produces output values between -1 and 1. The tanh function is similar to the sigmoid function, but it is symmetric around 0, which can be useful for certain types of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tSuppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "•\tWhat is the shape of the input matrix X?\n",
    "\n",
    "•\tWhat about the shape of the hidden layer’s weight vector Wh, and the shape of its bias vector bh?\n",
    "\n",
    "•\tWhat is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "\n",
    "•\tWhat is the shape of the network’s output matrix Y?\n",
    "\n",
    "•\tWrite the equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo and bo.\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The MLP has the following specifications:\n",
    "\n",
    "- Input layer: 10 passthrough neurons\n",
    "- Hidden layer: 50 artificial neurons with ReLU activation function\n",
    "- Output layer: 3 artificial neurons with ReLU activation function\n",
    "\n",
    "Here are the answers to the questions:\n",
    "\n",
    "-  The shape of the input matrix X is (m, 10), where m is the number of examples in the input dataset. Each example has 10 features that are input into the network.\n",
    "\n",
    "-  The shape of the hidden layer’s weight vector Wh is (10, 50). Each of the 10 input neurons is connected to each of the 50 hidden neurons, hence the weight matrix has a shape of (10, 50). The shape of the bias vector bh is (1, 50), where each of the 50 hidden neurons has its own bias term.\n",
    "\n",
    "-  The shape of the output layer’s weight vector Wo is (50, 3). Each of the 50 hidden neurons is connected to each of the 3 output neurons, hence the weight matrix has a shape of (50, 3). The shape of the bias vector bo is (1, 3), where each of the 3 output neurons has its own bias term.\n",
    "\n",
    "-  The shape of the network’s output matrix Y is (m, 3), where m is the number of examples in the input dataset, and 3 is the number of output neurons.\n",
    "\n",
    "-  The equation that computes the network’s output matrix Y as a function of X, Wh, bh, Wo, and bo is:\n",
    "\n",
    "Y = ReLU(ReLU(X @ Wh + bh) @ Wo + bo)\n",
    "\n",
    "Here, @ denotes matrix multiplication, and the ReLU function is applied elementwise to each element of the matrix. The first term in the parentheses represents the output of the hidden layer, which is computed by multiplying the input matrix X with the weight matrix Wh, and then adding the bias vector bh. The ReLU function is applied elementwise to the resulting matrix. The second term in the parentheses represents the output of the output layer, which is computed by multiplying the output of the hidden layer with the weight matrix Wo, and then adding the bias vector bo. The ReLU function is applied elementwise to the resulting matrix to obtain the final output of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tHow many neurons do you need in the output layer if you want to classify email into spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activation function?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "If you want to classify emails into spam or ham, you only need one neuron in the output layer. This neuron will output a value between 0 and 1, which can be interpreted as the probability of the email being spam. Typically, a threshold of 0.5 is used to classify the email as spam (if the output is greater than or equal to 0.5) or ham (if the output is less than 0.5). The activation function used in the output layer for binary classification tasks such as this is the sigmoid function, which maps any real-valued input to a value between 0 and 1.\n",
    "\n",
    "For the MNIST task, you need 10 neurons in the output layer, since there are 10 possible classes (the digits 0 to 9). The activation function used in the output layer for multiclass classification tasks such as this is the softmax function, which maps any vector of real numbers to a probability distribution over the classes. Each output neuron corresponds to a different digit, and the output of the network can be interpreted as the probabilities of the input image belonging to each of the 10 classes. The class with the highest probability is then selected as the predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tWhat is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Backpropagation is a popular algorithm for training artificial neural networks, particularly multi-layer perceptrons (MLPs). It is an optimization algorithm that uses gradient descent to minimize the loss function of the network with respect to its weights. Backpropagation works by computing the gradient of the loss function with respect to each weight in the network, and then updating the weights in the direction of the negative gradient to minimize the loss.\n",
    "\n",
    "Backpropagation is based on the chain rule of calculus, which allows us to compute the gradient of a composite function by chaining together the gradients of its constituent functions. In an MLP, backpropagation starts with the forward pass, where the input is passed through the network and the output is computed. The output is then compared to the target output, and the difference between the two is used to compute the loss function. The backward pass then computes the gradient of the loss function with respect to each weight in the network, using the chain rule to propagate the gradient backwards through the network. Finally, the weights are updated in the direction of the negative gradient using an optimization algorithm such as gradient descent.\n",
    "\n",
    "Reverse-mode autodiff is a more general technique for computing gradients of a function with respect to its inputs. It is based on the idea of breaking down a complex function into a sequence of simpler functions, and then computing the gradients of these simpler functions in reverse order. Reverse-mode autodiff is essentially the same as backpropagation, but without the network structure of an MLP. It can be used to compute gradients of any function, not just MLPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\tCan you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "There are several hyperparameters that can be tweaked in an MLP, including:\n",
    "- Number of layers\n",
    "- Number of neurons per layer\n",
    "- Learning rate\n",
    "- Activation function\n",
    "- Regularization strength (e.g., L1 or L2 regularization)\n",
    "- Dropout rate\n",
    "- Batch size\n",
    "- Number of epochs\n",
    "\n",
    "If the MLP overfits the training data, there are several hyperparameters that can be tweaked to try to solve the problem. Here are a few possible strategies:\n",
    "\n",
    "- Decrease the number of neurons in each layer to reduce the model complexity.\n",
    "- Increase the regularization strength to penalize large weights and reduce overfitting.\n",
    "- Increase the dropout rate to randomly drop out more neurons during training and reduce overfitting.\n",
    "- Increase the batch size to get a more representative sample of the training data during each training step.\n",
    "- Stop training earlier (i.e., decrease the number of epochs) to avoid overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\tTrain a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of an interruption, add summaries, plot learning curves using TensorBoard, and so on).\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
