{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Trainable parameters are model parameters that can be updated during the training process, such as weights and biases in a neural network. Non-trainable parameters are parameters that are set and remain constant during training, such as the shape of the input tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. In the CNN architecture, where does the DROPOUT LAYER go?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The dropout layer can be placed anywhere in the CNN architecture, typically between dense layers or after a pooling layer. The dropout rate can be tuned to achieve the best results for a particular task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What is the optimal number of hidden layers to stack?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The optimal number of hidden layers to stack in a CNN architecture depends on the complexity of the task and the size of the dataset. In general, deep networks with more hidden layers can learn more complex representations, but they also have a higher risk of overfitting the training data. Finding the optimal number of hidden layers often requires experimentation and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. In each layer, how many secret units or filters should there be?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The number of filters or neurons in each layer can also be tuned for optimal performance. A common approach is to start with a small number of filters and then increase the number as the network deepens. The optimal number of filters will depend on the complexity of the task and the size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What should your initial learning rate be?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The initial learning rate is a hyperparameter that determines the step size at which the model parameters are updated during training. The optimal learning rate can vary greatly depending on the task and the model architecture, and it is often set through a process of experimentation and fine-tuning. A learning rate that is too high can result in instability and slow convergence, while a learning rate that is too low may result in slow training. A common approach is to start with a relatively high learning rate and then reduce it over time as the model converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What do you do with the activation function?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The activation function is an important component of a neural network, as it is responsible for introducing non-linearity into the model. The activation function is applied element-wise to the output of each neuron, and common activation functions include the sigmoid function, the hyperbolic tangent (tanh) function, and the rectified linear unit (ReLU) function. The choice of activation function can have a significant impact on model performance, and it is often necessary to experiment with different activation functions to find the best one for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. What is NORMALIZATION OF DATA?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Normalization of data refers to the process of transforming the values of a dataset so that it has a mean of zero and a standard deviation of one. This helps to ensure that the data is well-conditioned, which in turn can improve the stability and performance of machine learning algorithms. Common normalization techniques include mean normalization, min-max scaling, and z-score normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What is IMAGE AUGMENTATION and how does it work?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Image augmentation is a technique used to artificially increase the size of a training dataset by transforming existing images in various ways. This can help to reduce overfitting, as the model will be exposed to a greater variety of image variations during training. Common image augmentation techniques include rotation, translation, scaling, flipping, and adding random noise to the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. What is DECLINE IN LEARNING RATE?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Decline in learning rate refers to a schedule for decreasing the learning rate over time during training. This is often used to help the model converge to a better solution, as a smaller learning rate allows the model to make smaller, more refined updates to its weights as it approaches a minimum in the loss surface. There are various strategies for implementing a decline in learning rate, including step decay, where the learning rate is decreased after a fixed number of epochs, and exponential decay, where the learning rate decreases exponentially over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. What does EARLY STOPPING CRITERIA mean?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Early stopping criteria refers to a condition that is used to determine when to stop training a model. This is often used to prevent overfitting, as a model that continues to be trained beyond the point where it has achieved its best performance on the validation data will start to memorize the training data and perform poorly on unseen data. Common early stopping criteria include monitoring the performance of the model on a validation set and stopping training when the performance stops improving, or when the performance on the validation set starts to degrade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
