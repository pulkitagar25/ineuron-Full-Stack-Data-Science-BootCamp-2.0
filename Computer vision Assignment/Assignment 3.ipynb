{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. After each stride-2 conv, why do we double the number of filters?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "After each stride-2 convolution, the number of filters is often doubled to compensate for the reduction in spatial dimensions and increase the capacity of the model to learn complex features. This is because when the spatial dimensions are reduced by half, the number of activations in the feature map is also reduced by half, which can lead to a loss of information. Doubling the number of filters helps to mitigate this loss of information by increasing the number of activations in the feature map, which allows the model to capture more information from the input.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In the MNIST (with simple cnn) example, a larger kernel (e.g., 5x5 or 7x7) is used in the first convolution to capture large features or patterns in the input image that can be used to distinguish between different classes. This is because the MNIST dataset consists of small 28x28 images, and a larger kernel allows the model to capture more information from the input.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What data is saved by ActivationStats for each layer?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "ActivationStats saves the activation statistics, such as mean, standard deviation, minimum, and maximum, for each layer in the network. These statistics can be used to monitor the behavior of the model during training and to diagnose any issues that may arise. By tracking these statistics, you can gain insights into how the model is learning and whether it is overfitting or underfitting the data, which can help you make informed decisions about how to modify the model to improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How do we get a learner's callback after they've completed training?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "In most machine learning frameworks, you can get a learner's callback after they have completed training by using the callback API. The callback API allows you to define functions that will be executed at specific stages during the training process, such as after each epoch or after the model has completed training. You can use this API to specify what actions should be taken when the model has finished training, such as saving the model weights or logging performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What are the drawbacks of activations above zero?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The main drawback of activations above zero is that they can lead to the vanishing gradients problem, which occurs when the gradients of the activations become very small during backpropagation. This can make it difficult for the model to learn, as the gradients are used to update the model weights during training. To address this issue, many activation functions are designed to have outputs that range from zero to one or from negative one to one, which helps to prevent the vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Draw up the benefits and drawbacks of practicing in larger batches?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The benefits of practicing in larger batches include faster training times and better use of hardware resources, as the model can make use of parallel processing and vectorized operations to perform multiple updates to the model weights in a single step. However, training in larger batches can also result in suboptimal convergence, as the model may not receive as much exposure to individual examples in the dataset. This can result in a trade-off between speed and accuracy, and it may be necessary to experiment with different batch sizes to find the best balance for your specific use case.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Why should we avoid starting training with a high learning rate?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "It is generally recommended to avoid starting training with a high learning rate because if the learning rate is set too high, the model's weights may be updated too quickly and the optimization process may become unstable. This can result in the model overshooting the optimal solution or oscillating back and forth between different suboptimal solutions. Starting with a low learning rate and gradually increasing it as training progresses can help to stabilize the optimization process and ensure that the model converges to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What are the pros of studying with a high rate of learning?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The main pro of training with a high learning rate is that it can lead to faster convergence, as the model's weights are updated more quickly. This can be beneficial when training complex models or working with large datasets, as it can allow you to train the model more efficiently and reach a good solution in a shorter amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Why do we want to end the training with a low learning rate?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "It is generally recommended to end training with a low learning rate because as the model approaches convergence, it becomes increasingly sensitive to the learning rate. If the learning rate is set too high, the model's weights may be updated too quickly and the optimization process may become unstable. Decreasing the learning rate as training progresses helps to ensure that the model converges to a good solution and prevents overshooting or oscillating between different suboptimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
