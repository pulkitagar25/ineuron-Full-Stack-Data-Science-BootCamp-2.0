{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How can each of these parameters be fine-tuned?\n",
    "\n",
    "• Number of hidden layers \n",
    "• Network architecture (network depth)\n",
    "\n",
    "• Each layer's number of neurons (layer width)\n",
    "\n",
    "• Form of activation\n",
    "\n",
    "• Optimization and learning\n",
    "\n",
    "• Learning rate and decay schedule\n",
    "\n",
    "• Mini batch size\n",
    "\n",
    "• Algorithms for optimization\n",
    "\n",
    "• The number of epochs (and early stopping criteria)\n",
    "\n",
    "• Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "• L2 normalization\n",
    "\n",
    "• Drop out layers\n",
    "• Data augmentation\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Fine-tuning each of these parameters can help improve the performance of a neural network.\n",
    "\n",
    "- Number of hidden layers: The number of hidden layers can be adjusted to find the optimal number that best balances model capacity and complexity. A larger number of hidden layers can increase model capacity, but may also lead to overfitting.\n",
    "\n",
    "- Network architecture (network depth): The architecture of a neural network can be altered to add or remove layers, change layer types, and adjust the number of neurons in each layer to optimize performance.\n",
    "\n",
    "- Each layer's number of neurons (layer width): The number of neurons in each layer can be adjusted to increase or decrease model capacity, with more neurons allowing for more complex representations but also increasing the risk of overfitting.\n",
    "\n",
    "- Form of activation: The activation function used in each layer can be changed to determine which form works best for the task at hand. Common activation functions include ReLU, sigmoid, and tanh.\n",
    "\n",
    "- Optimization and learning: Different optimization algorithms can be used to update model weights, such as stochastic gradient descent (SGD), Adam, or RProp.\n",
    "\n",
    "- Learning rate and decay schedule: The learning rate is a hyperparameter that controls the step size of weight updates during training. A decay schedule can be applied to gradually reduce the learning rate over time, helping the model converge to a stable solution.\n",
    "\n",
    "- Mini batch size: The mini batch size can be adjusted to control the trade-off between training speed and optimization. Smaller mini batch sizes can lead to faster training times but also increased variance in weight updates, while larger mini batch sizes can lead to slower training times but better optimization.\n",
    "\n",
    "- Algorithms for optimization: Different optimization algorithms can be used to update model weights, such as stochastic gradient descent (SGD), Adam, or RProp.\n",
    "\n",
    "- The number of epochs (and early stopping criteria): The number of training epochs can be adjusted to find the optimal number that balances training time and model performance. Early stopping criteria can be used to stop training when the model performance on a validation set stops improving.\n",
    "\n",
    "- Overfitting can be avoided by using regularization techniques: Regularization techniques, such as L2 normalization or dropout, can be applied to a neural network to reduce overfitting and improve generalization.\n",
    "\n",
    "- L2 normalization: L2 normalization is a type of regularization that adds a penalty term to the loss function based on the sum of squares of model weights, encouraging the model to have smaller weight values.\n",
    "\n",
    "- Drop out layers: Dropout is a regularization technique that randomly sets some neurons in a layer to zero during training, helping to reduce overfitting by encouraging the model to learn more robust representations.\n",
    "\n",
    "- Data augmentation: Data augmentation techniques, such as random cropping, flipping, and rotation, can be used to artificially increase the size of the training dataset, helping to reduce overfitting by encouraging the model to learn more robust representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
