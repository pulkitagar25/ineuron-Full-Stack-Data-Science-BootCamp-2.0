{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What are the advantages of a CNN for image classification over a completely linked DNN?\n",
    "\n",
    "Ans=>\n",
    "\n",
    "One of the main advantages of CNNs for image classification over a fully connected DNN is that CNNs have a smaller number of parameters. This reduces the risk of overfitting and makes the model easier to train. Additionally, CNNs use a local receptive field, which allows them to capture the spatial structure of the input image. This is particularly useful for image classification, as objects in an image can appear at different scales and positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two, and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does the CNN have in total? How much RAM would this network need when making a single instance prediction if we're using 32-bit floats? What if you were to practice on a batch of 50 images?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "The number of parameters in each convolutional layer can be calculated as follows:\n",
    "\n",
    "- Number of parameters in one kernel = (width * height * number of input channels + 1) * number of output channels\n",
    "- Number of parameters in one layer = number of kernels * number of parameters in one kernel\n",
    "- Total number of parameters in the three layers = (number of parameters in one layer) * 3\n",
    "\n",
    "Assuming a kernel size of 3 x 3, the number of parameters in one kernel of the bottom layer is (3 * 3 * 3 + 1) * 100 = 2700. The number of parameters in one layer is 100 * 2700 = 270,000.\n",
    "\n",
    "The number of parameters in one kernel of the middle layer is (3 * 3 * 100 + 1) * 200 = 18,000. The number of parameters in one layer is 200 * 18,000 = 3,600,000.\n",
    "\n",
    "The number of parameters in one kernel of the top layer is (3 * 3 * 200 + 1) * 400 = 72,000. The number of parameters in one layer is 400 * 72,000 = 28,800,000.\n",
    "\n",
    "The total number of parameters in the three layers is 270,000 + 3,600,000 + 28,800,000 = 32,670,000.\n",
    "\n",
    "For a single instance prediction, the network would need approximately 32,670,000 * 4 bytes/parameter = 130,680,000 bytes of RAM to store the parameters, which is approximately 123 MB.\n",
    "\n",
    "For a batch of 50 images, the network would need approximately 50 * 123 MB = 6.15 GB of RAM to store the parameters. Note that this calculation only considers the memory required for storing the parameters and doesn't take into account the memory required for storing intermediate activations during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What are five things you might do to fix the problem if your GPU runs out of memory while training a CNN?\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "If your GPU runs out of memory while training a CNN, here are five things you can try:\n",
    "\n",
    "- Reduce the batch size, as this will reduce the memory required for each forward and backward pass.\n",
    "- Reduce the complexity of the network architecture, such as reducing the number of filters or the size of the filters.\n",
    "- Use half-precision (float16) data types instead of single-precision (float32) to reduce memory usage.\n",
    "- Use gradient checkpointing to reduce the memory required to store intermediate activations.\n",
    "- If possible, use a GPU with more memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Max pooling is often used with a convolutional layer of the same stride to reduce the spatial dimensions of the feature map, resulting in a reduced computation time and reduced risk of overfitting. Additionally, max pooling can also help to preserve the important features in the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. When would a local response normalization layer be useful?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "A local response normalization layer can be useful in cases where the input images have large variations in contrast and intensity. The normalization layer helps to reduce the impact of these variations and increase the stability of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and ResNet's core innovations?\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "Some other potential use cases for local response normalization layers include improving the robustness of the network to small changes in the input, reducing the sensitivity of the network to noise, and improving the generalization ability of the network to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. On MNIST, build your own CNN and strive to achieve the best possible accuracy.\n",
    "\n",
    "Ans=>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "844/844 [==============================] - 37s 37ms/step - loss: 0.2058 - accuracy: 0.9373 - val_loss: 0.0555 - val_accuracy: 0.9835\n",
      "Epoch 2/10\n",
      "844/844 [==============================] - 32s 38ms/step - loss: 0.0744 - accuracy: 0.9771 - val_loss: 0.0380 - val_accuracy: 0.9890\n",
      "Epoch 3/10\n",
      "844/844 [==============================] - 33s 39ms/step - loss: 0.0555 - accuracy: 0.9825 - val_loss: 0.0325 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "844/844 [==============================] - 30s 36ms/step - loss: 0.0443 - accuracy: 0.9856 - val_loss: 0.0313 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "844/844 [==============================] - 30s 35ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.0315 - val_accuracy: 0.9912\n",
      "Epoch 6/10\n",
      "844/844 [==============================] - 30s 35ms/step - loss: 0.0336 - accuracy: 0.9890 - val_loss: 0.0302 - val_accuracy: 0.9907\n",
      "Epoch 7/10\n",
      "844/844 [==============================] - 29s 34ms/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 0.0282 - val_accuracy: 0.9928\n",
      "Epoch 8/10\n",
      "844/844 [==============================] - 29s 35ms/step - loss: 0.0270 - accuracy: 0.9909 - val_loss: 0.0287 - val_accuracy: 0.9920\n",
      "Epoch 9/10\n",
      "844/844 [==============================] - 29s 34ms/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.0277 - val_accuracy: 0.9922\n",
      "Epoch 10/10\n",
      "844/844 [==============================] - 29s 35ms/step - loss: 0.0233 - accuracy: 0.9921 - val_loss: 0.0263 - val_accuracy: 0.9932\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0216 - accuracy: 0.9927\n",
      "Test accuracy: 0.9926999807357788\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Using Inception v3 to classify broad images.\n",
    "\n",
    "a.Images of different animals can be downloaded. Load them in Python using the matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency. The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to 1.0, so make sure yours do as well.\n",
    "\n",
    "Ans=>\n",
    "\n",
    "- Split the images into training and validation sets. The validation set will be used to monitor overfitting, or the extent to which the model is adapting too well to the training data, and not generalizing well enough to new data.\n",
    "\n",
    "- Use the pre-trained Inception v3 model, which is available in TensorFlow's Keras library, to classify the images. The pre-trained model can be used as a feature extractor, or fine-tuned to adapt to the new task of animal classification. When using the model as a feature extractor, the final dense layer is replaced with a new dense layer trained on the new task.\n",
    "\n",
    "- Train the model on the training set, using data augmentation to generate more training data by flipping, rotating, and zooming images. This helps to reduce overfitting and increase the model's robustness.\n",
    "\n",
    "- Evaluate the model on the validation set, and monitor the accuracy and loss to determine if the model is overfitting or underfitting. If overfitting, try reducing the complexity of the model by removing layers or increasing the dropout rate. If underfitting, try increasing the complexity of the model by adding layers or increasing the number of neurons.\n",
    "\n",
    "- Once satisfied with the model's performance on the validation set, use it to make predictions on new images of animals, or use it to classify new images in a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Large-scale image recognition using transfer learning.\n",
    "a. Make a training set of at least 100 images for each class. You might, for example, identify your own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as the flowers dataset or MIT's places dataset (requires registration, and it is huge).\n",
    "\n",
    "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also adding some randomness for data augmentation.\n",
    "\n",
    "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the last layer before output layer) and replace output layer with  appropriate number of outputs for your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the output layer must have five neurons and use softmax activation function).\n",
    "\n",
    "d. Separate the data into two sets: a training and a test set. The training set is used to train the model, and the test set is used to evaluate it.\n",
    "\n",
    "\n",
    "\n",
    "Ans=>\n",
    "\n",
    "- Train the model for a number of epochs (iterations over the entire dataset) using a suitable optimizer, such as Adam or SGD, and a suitable loss function, such as categorical cross-entropy. You can also experiment with different batch sizes, learning rates, and other hyperparameters to see if they affect the model's accuracy.\n",
    "\n",
    "- Evaluate the model's performance on the test set by measuring its accuracy, precision, recall, and F1 score. You can also create confusion matrices and ROC curves to visualize the model's performance.\n",
    "\n",
    "- Fine-tune the model by unfreezing some of the layers and retraining them. You can experiment with unfreezing different numbers of layers and different parts of the network to see if it improves performance.\n",
    "\n",
    "- Finally, use the trained model to make predictions on new images and evaluate its accuracy. You can also use the model to generate predictions for all images in the dataset and compare them with the actual labels to see if it's working well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
